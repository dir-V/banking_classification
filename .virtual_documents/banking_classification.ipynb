import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc


# Importing Data
bank = pd.read_csv('bank-additional-full.csv', delimiter= ';')


bank.head(5)


#Checking Data
bank.info()


#Checking for null values within data
bank.isnull().sum()


bank.describe()


bank.columns


bank.hist(figsize=(20, 10))


# Checking Correlation in Variables
plt.figure(figsize=(20,10))
sns.heatmap(bank.corr(), annot=True, cmap='YlGnBu')


# Removing Variables that are highly correlated (0.95 or more)
bank_processed = bank.drop(['euribor3m', 'nr.employed','emp.var.rate'], axis=1)
bank_processed.head(10)


#Converting Age Columns into ranges instead of discrete values
bank_processed['age_range'] = pd.cut(bank_processed['age'], bins = 5, labels = [f'{i+1}'for i in range(5)])
bank_processed.head(10)


#Checking the Ranges of each "Group" created
print(bank_processed.groupby('age_range')['age'].min())
print(bank_processed.groupby('age_range')['age'].max())


#Converting Categorical Columns into Numerical for Analysis

from sklearn.preprocessing import LabelEncoder
label_enc = LabelEncoder()
columns = ['job','marital', 'education', 'default', 'contact', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'campaign', 'pdays', 'poutcome', 'y', 'age_range']
for column in columns:
    bank_processed[column] = label_enc.fit_transform(bank_processed[column])
bank_processed.head(10)


#Dropping age column as no longer needed

bank_processed = bank_processed.drop(['age'], axis = 1)
bank_processed.head(10)


#Ensuring all columns have full integrity
bank_processed.isnull().any()


#Checking correlation again

plt.figure(figsize=(20,10))
sns.heatmap(bank_processed.corr(), annot=True, cmap='YlGnBu')


from sklearn.model_selection import train_test_split

#Splitting Test and Train Data
X_data = bank_processed.drop(['y'], axis = 1)
y_data = bank_processed['y']

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state = 37)

#Checking the shapes are correct
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)


from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()

X_train = scalar.fit_transform(X_train)
X_test = scalar.transform(X_test)


from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()

dtc.fit(X_train, y_train)
y_pred_dtc = dtc.predict(X_test)

accuracy_dtc = accuracy_score(y_test, y_pred_dtc)
print(accuracy_dtc)

cm_dtc = confusion_matrix(y_test, y_pred_dtc)

plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - DTC")
sns.heatmap(cm_dtc, annot=True, fmt='d',  cmap='Purples')
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(accuracy_lr)
cm_lr = confusion_matrix(y_test, y_pred_lr)

plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - Linear Regression")
sns.heatmap(cm_lr, annot=True, fmt='d',  cmap='Purples', cbar=False)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

rfc.fit(X_train, y_train)
y_pred_rfc = rfc.predict(X_test)

accuracy_rfc = accuracy_score(y_test, y_pred_rfc)
print(accuracy_rfc)
cm_rfc = confusion_matrix(y_test, y_pred_rfc)

plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - RFC")
sns.heatmap(cm_rfc, annot=True, fmt='d',  cmap='Purples', cbar=False)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(accuracy_knn)
cm_knn = confusion_matrix(y_test, y_pred_knn)

plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - KNN")
sns.heatmap(cm_knn, annot=True, cmap='Purples', fmt='d', cbar=False)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.svm import SVC

svm = SVC(kernel='rbf', probability=True)

svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(accuracy_svm)

cm_svm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - SVM")
sns.heatmap(cm_svm, annot=True, cmap='Purples', fmt='d', cbar=False)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()

gbc.fit(X_train, y_train)
y_pred_gbc = gbc.predict(X_test)

accuracy_gbc = accuracy_score(y_test, y_pred_gbc)
print(accuracy_gbc)

cm_gbc = confusion_matrix(y_test, y_pred_gbc)
plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - Gradient Boosting")
sns.heatmap(cm_gbc, annot=True, cmap='Purples', fmt='d', cbar=False)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()

gnb.fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)

accuracy_gnb = accuracy_score(y_test, y_pred_gnb)
print(accuracy_gnb)

cm_gnb = confusion_matrix(y_test, y_pred_gnb)
plt.figure(figsize=(6, 4))
plt.title("Confusion Matrix - Naive Bayes")
sns.heatmap(cm_gnb, annot=True, cmap='Purples', fmt='d', cbar=False)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")


from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)

models = {
    "Decision Tree": dtc,
    "Logistic Regression": lr,
    "Random Forest": rfc,
    "K Nearest Neighbors": knn,
    "Support Vector Machines": svm,
    "Gradient Boosting": gbc,
    "Naive Bayes": gnb
     }
preds = {
    "Decision Tree": y_pred_dtc,
    "Logistic Regression": y_pred_lr,
    "Random Forest": y_pred_rfc,
    "K Nearest Neighbors": y_pred_knn,
    "Support Vector Machines": y_pred_svm,
    "Gradient Boosting": y_pred_gbc,
    "Naive Bayes": y_pred_gnb
    }


for modelName, y_pred in preds.items():
    print(f"{modelName}: ", classification_report(y_test, y_pred))


for modelName, model in models.items():
    cvs = cross_val_score(model, X_train, y_train, cv=kf).mean()
    print(f"{modelName}: ", cvs)


from sklearn import metrics
for modelName, model in models.items():
    if hasattr(model, "predict_proba"):
        y_probs = model.predict_proba(X_test)[:, 1]
    else:
        y_probs = model.decision_function(X_test)
    fpr, tpr, thresholds = roc_curve(y_test, y_probs)
    roc_auc = metrics.auc(fpr, tpr)
    plt.figure(figsize =(5, 5))
    plt.plot(fpr, tpr, linestyle="-", label=f'{modelName}, (auc = %0.3f)' % roc_auc)
    plt.plot([0, 1], [0, 1], linestyle="--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Negative Rate")
    plt.legend()
    plt.tight_layout()
    plt.show()


# Feature Importance of Gradient Boosting
fi_gb_df = pd.DataFrame({'Feature': X_data.columns, 'Importance': gbc.feature_importances_})
fi_gb_df = fi_gb_df.sort_values(by='Importance', ascending=False)
print(fi_gb_df)


#Feature Importance of Random Forest Classifier
fi_rfc_df = pd.DataFrame({'Feature': X_data.columns, 'Importance': rfc.feature_importances_})
fi_rfc_df = fi_rfc_df.sort_values(by='Importance', ascending=False)
print(fi_rfc_df)



